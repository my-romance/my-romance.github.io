---
layout: post
title:  "토픽 모델링(Topic Modeling) : 잠재 디리클레 할당(Latent Dirichelt Allocation)"
date:   2020-05-12
author: Romance
categories: NLP
---
# 잠재 딜리클레 할당 (Latent Dirichlet Allocation)

토픽 모델링은 문서의 집합에서 **topic** 을 찾아내는 프로세스를 의미. 검색엔진, 고객 민원 시스템 등과 같이 **문서의 주제**를 알아내는 일이 중요한 곳에서 사용됨.

LDA의 아이디어는 `문서들은 토픽들의 혼합으로 구성되고, 토픽들은 확률 분포에 기반하여 단어들을 생성`이다. 데이터가 주어지면, LDA는 문서가 생성되던 과정을 역추정.

------



### LDA 개요

<문서 예시>

문서 1 : 저는 사과랑 바나나를 먹어요

문서 2 : 우리는 귀여운 강아지가 좋아요

문서 3 : 저의 깜찍하고 귀여운 강아지가 바나나를 먹어요

<각 문서의 토픽 분포>

문서 1 : 토픽 A 100%

문서 2 : 토픽 B 100%

문서 3 : 토픽 A 40%, 토픽 B 60%

<각 토픽의 단어 분포>

토픽A : **사과 20%, 바나나 40%, 먹어요 40%**, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%
토픽B : 사과 0%, 바나나 0%, 먹어요 0%, **귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%**

------



### LDA의 가정

LDA는 빈도수 기반의 표현 방법인 DTM, TF-IDF 행렬을 입력으로 하므로, LDA는 단어의 순서는 고려하지 않음



**LDA의 과정**

1. 문서에 사용할 단어의 개수 N을 정함.
2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정.
3. 문서에 사용할 각 단어를 (아래와 같이) 정함.
   1. 토픽 분포에서 토픽 T를 확률적으로 고름
      Ex) 60% 확률로 강아지 토픽을 선택하고, 40%의 확률로 과일 토픽을 선택할 수 있음
   2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반한 문서에 사용할 단어를 고름

4. 3 과정을 반복하면서 문서를 완성.

이러한 과정을 통해 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위해 이 과정을 역으로 추적하는 역공학을 수행함.

------



### LDA의 수행하기

1. 사용자는 알고리즘에게 토픽의 개수 k를 알려줌.
2. 모든 단어를 k개 중 하나의 토픽에 할당시킴.
3. 모든 문서의 모든 단어에 대해 아래의 사항을 반복 진행.
   -  **어떤 문서의 각 단어 w는 자신을 잘못된 토픽에 할당되어 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정.** 이에 따라 단어 w는 아래의 두 가지 기준에 따라 **토픽이 재할당됨.**
     - $p(topic t | document d)$ : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율.
     - $p(word w | topic t)$ : 단어 w를 갖고 있는 모든 문서들 중 토픽 t가 할당된 비율.
   - 3번 과정을 반복하면, 모든 할당이 완료된 수렴상태가 됨. 

------



### LSA와 LDA의 차이점 !! 

- LSA : DTM을 차원 축소하여, 축소 차원에서 근접 단어들을 토픽으로 묶음. → 단어의 잠재적인 의미를 이끌어낼 수 있도록 하였음. 즉, 한 단어를 dense vector로 표현할 수 있음.(But, Word2vec 성능이 더 좋다.)
- LDA : **문서에 특정 토픽이 존재할 확률과 단어가 특정 토픽에 존재할 확률을 결합확률로 추정하여 토픽을 추출.** → 문서 별 토픽 분포와 토픽 별 단어 분포를 알 수 있음.

------



### 참조자료

- http://wikidocs.net/30708
- https://wikidocs.net/40710
