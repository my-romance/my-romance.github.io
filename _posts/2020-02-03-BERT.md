---
layout: post
title:  "BERT"
date:   2020-02-03
author: Romance
categories: NLP
---
## BERT
### 1. BERT, ELMo, GPT

- BERT, ELMo, GPT architecture 차이점	
<img src="/assets/image/BERT_1.png">

  - GPT : 단어 시퀀스를 왼쪽에서 오른쪽으로 한 방향으로만 보는 모델 ← 언어모델이기 때문에
  - ELMo : Bi-LSTM 레이어의 상단은 양방향이지만 중간 레이어는 한 방향인 모델
  - BERT : 모든 레이어에서 양방향 성질을 잃지 않는 모델

- 양방향, 단방향 언어모델

  - 단방향 언어모델 : `나는 어제 ______ ` 

                             →

    - 기존 언어 모델 : 주어진 시퀀스 다음 단어를 예측하는 과정에서 학습

  - 양방향 언어모델 : `나는 어제 ______ 먹었다 `

                             →           ←

    - 마스크 언어 모델 (BERT) : 문장 전체를 모델에 알려주고, 빈칸(MASK)에 해당하는 단어가 어떤 단어일지 얘측하는 과정에서 학습

      
<br>

### 2. 프리트레인 태스크

프리트레인 태스크 방법 2가지 :  마스크 언어 모델, 다음 문장인지 여부 맞추기(NSP:Next_Sentence_prediction)

- 마스크 언어 모델

  `발 없는 말이 [MASK] 간다` →  천리

  - 기대 효과 :
    - `발 없는 말이 [MASK] 간다`의 빈칸을 채워야 하기 때문에 문장 내 어느 자리에 어떤 단어를 쓰는게 자연스러운지 앞뒤 문맥 파악 가능
    - `발 없는 말이 천리 간다`, `발 없는 말이 컴퓨터 간다`를 비교해 보면서 주어진 문장이 의미/문법상 비문인지 아닌지 가려낼 수 있다
    - 모델은 어떤 단어가 마스킹될지 전혀 모르기 때문에 문장 내 모든 단어 사이의 의미적, 문법적 관계를 세밀히 살피게 된다

- NSP

  `애비는 종이였다. 밤이 깊어도 오지 않았다.` →  참(True)

  - 기대 효과 : 
    - 모델은 `애비는 종이었다`, `밤이 깊어도 오지 않았다`가 이어진 문장인지 아닌지 반복 학습 → 문장 간 의미 관계 파악 가능
    - 일부 문장 성분이 없어도 전체 의미를 파악하는데 큰 무리 X ← NSP 태스크가 너무 쉬워지는 것을 방지하기 위해 문장 맨 앞 또는 맨 뒤쪽 단어 일부를 삭제 하였기에
    - 학습데이터에 짧은 문장이 포함돼 있어도 성능이 크게 떨어지지 X ← 학습데이터의 10%는 사용자가 정한 최대 길이보다 짧은 데이터로 구성되어 있기에
<br>

### 3. BERT 모델의 구조

- BERT의 입력 레이어	!
<img src="/assets/image/BERT_2.PNG">
  - 토큰 임베딩 : 입력 토큰에 해당하는 토큰 벡터를 참조한 임베딩
  - 세그먼트 임베딩 : 첫번째 문장인지, 두번째 문장인지에 해당하는 임베딩
  - 포지션 임베딩 : 입력 토큰의 문장 내 절대적인 위치에 해당하는 임베딩

- 활성함수 : GELU 사용 → 정규분포의 누적분포함수인 GELU는 ReLU보다 0 주위에서 부드럽게 변화해 학습성능 ↑


<img src="/assets/image/BERT_3.png">

- 예측 레이어 사용 →  마스크 언어모델, NSP 과제를 수행하기 위함.

  - 예측 레이어의 입력 : 마지막 트랜스포머 블록의 마스크 위치에 해당하는 토큰 벡터

    `발 없는 말이 [MASK] 간다` →  네 번째 벡터가 input_tensor (띄어쓰기 기준으로 토큰을 나눈다면)

  - 예측 레이어를 통해 로짓(logit) 벡터를 만듦


<br>
#### 참조

한국어 임베딩 책

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 논문 
