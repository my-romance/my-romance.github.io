---
layout: post
title:  "텍스트 전처리 : 정제(Cleaning)와 정규화(Normalization)"
date:   2020-03-03
author: Romance
categories: NLP
---
# 텍스트 전처리 : 정제(Cleaning)와 정규화(Normalization)

**정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거**. 정제 작업은 토큰화 적업에 방해가 되는 부분들을 배제시키기 위해 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기  위해 지속적으로 이루어짐

**정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦**

<br>

### 규칙에 기반한, 표기가 다른 단어들의 통합

- 정규화 규칙을 이용하여, **같은 의미를 가지지만 표기가 다른 단어들을 하나의 단어로 정규화**함

  Example :

  - USA와 US  → 정규화를 통해 US를 찾을 때 USA도 함께 찾을 수 있도록 해야함
  - uh-huh와 uhhuh

- 표기가 다른 단어들을 통합하는 대표 방법
  - 어간 추출(stemming)
  - 표제어 추출(lemmatizaiton)

<br>  

### 대, 소문자 통합

영어권 언어에서 **대, 소문자를 통합**하는 것은 단어의 개수를 줄일 수 있는 하나의 **정규화 방법**

<br>

### 불필요한 단어의 제거

정제 작업에서의 노이즈 데이터 : 

1. 자연어가 아니면서 **아무 의미도 갖지 않는 글자**(특수 문자 등)을 의미

2. 분석하고자 하는 **목적에 맞지 않는 불필요한 단어**를 의미
   

- **불필요한 단어를 제거**하는 방법
  1. 등장 빈도가 적은 단어
  2. 길이가 짧은 단어
  3. 불용어 (이후에 자세히 정리)

- **등장 빈도가 적은 단어 (Removing Rare words)**

  텍스트 데이터에서 너무 적게 등장하여 도움이 되지 않는 단어

  Example : 100,000개의 문서 중 총 5번만 등장한 단어 → 직관적으로 분류와 같은 자연어처리 task에 도움이 되지 않음.

- **길이가 짧은 단어 (Removing words with very a short length)**

  - 영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도 어느 정도 자연어 처리에서 **크게 의미가 없는 단어들을 제거하는 효과**를 볼 수 있음 → 길이가 2~3 이하인 단어 제거

  - 하지만 한국에서는 이런 방법이 유효하지 않기도 함 ← 영어 단어의 평균길이는 6\~7, 한국어 단어의 평균 길이는 2\~3. 즉, 한 글자가 가진 의미의 크기가 다름

<br>

### 정규 표현식

- 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면 → **노이즈 데이터를 제거하기 위해 정규 표현식을 사용**하는 것이 유용

<br>

### 출처

- https://wikidocs.net/21693




